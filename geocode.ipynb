{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "\n",
    "import geopandas as gpd\n",
    "import configparser\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config file contains file paths and name of township (uesd to match parcel TWP_CITY column)\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Change this path to the location of your own config.ini file\n",
    "config.read('/home/petermitchell/Documents/ContractWork/config.ini')\n",
    "config.sections()\n",
    "\n",
    "# Set variables to inputs from config file\n",
    "twp = config['DEFAULT']['township']\n",
    "verified_wells_path = config['DEFAULT']['verified_wells']\n",
    "unverified_wells_path = config['DEFAULT']['unverified_wells']\n",
    "parcels_path = config['DEFAULT']['parcels']\n",
    "samples_path = config['DEFAULT']['samples']\n",
    "output_path = config['DEFAULT']['output']\n",
    "\n",
    "output_dir = Path(output_path)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload csv to us census geocoder. As an alternative, you can just go to the url below and upload it yourself\n",
    "# File cannot exceed 10,000 records\n",
    "url = 'https://geocoding.geo.census.gov/geocoder/geographies/addressbatch'\n",
    "files = {'addressFile': (samples_path, open(samples_path, 'rb'), 'text/csv')}\n",
    "payload = {'benchmark':'Public_AR_Current', 'vintage':'Current_Current'}\n",
    "print('Posting requests...')\n",
    "s = requests.post(url, files=files, data=payload)\n",
    "print(type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(io.StringIO(s.text), sep=',', header=None, quoting=csv.QUOTE_ALL, keep_default_na=False, names = ['id', 'address_in', 'match_indicator', 'match_type', 'address_out', 'long_lat', 'tiger_edge', 'street_side', 'FIPS_STATE', 'FIPS_COUNTY', 'CENSUS_TRACT', 'CENSUS_BLOCK'])\n",
    "\n",
    "#df = pd.read_csv('/home/petermitchell/Downloads/GeocodeResults(1).csv', names = ['id', 'address_in', 'match_indicator', 'match_type', 'address_out', 'long_lat', 'tiger_edge', 'street_side', 'FIPS_STATE', 'FIPS_COUNTY', 'CENSUS_TRACT', 'CENSUS_BLOCK'])\n",
    "pd.DataFrame.head(df)\n",
    "\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['long', 'lat']] = df['long_lat'].str.split(',', expand=True)\n",
    "out = df[['id', 'long', 'lat', 'address_out']]\n",
    "out.to_csv(output_dir / '911_addresses_out.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = gpd.read_file(parcels_path).to_crs(26915)\n",
    "\n",
    "\n",
    "# Limit parcels to just those in the desired township\n",
    "#twp_parcels = parcels[parcels['TWP_CITY'] == twp]\n",
    "\n",
    "parcels['Parcel_Township'] = parcels['TWP_CITY'].str.removesuffix(' TOWNSHIP')\n",
    "\n",
    "verified_wells = gpd.read_file(verified_wells_path)\n",
    "unverified_wells = gpd.read_file(unverified_wells_path)\n",
    "\n",
    "verified_wells['Verified'] = True\n",
    "unverified_wells['Verified'] = False\n",
    "\n",
    "# Join verified and unverified wells into a single dataframe\n",
    "wells = gpd.GeoDataFrame(pd.concat([verified_wells, unverified_wells], ignore_index=True))\n",
    "\n",
    "# Export all wells\n",
    "#wells.to_file(output_dir / \"wells.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set samples variable to the output of the geocoding process\n",
    "samples = out\n",
    "samples_out = samples.copy()\n",
    "samples_out.set_index('Lab_SampleID', inplace=True)\n",
    "samples_out['Confidence_Code'] = pd.Series(dtype='int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address_1 field in parcels contains the RR address\n",
    "\n",
    "# Join on ADDRESS_1, rr_addresses\n",
    "\n",
    "samples = samples.replace('', pd.NA).dropna(subset=['lat', 'long'])\n",
    "located_samples = gpd.GeoDataFrame(samples, geometry=gpd.points_from_xy(samples[\"long\"], samples[\"lat\"], crs=\"EPSG:4326\")).to_crs(26915)\n",
    "print(len(located_samples))\n",
    "print(located_samples.crs)\n",
    "\n",
    "located_samples.to_file(\"test_samples.gpkg\")\n",
    "located_samples = gpd.sjoin(parcels, located_samples, how=\"inner\", predicate=\"intersects\")\n",
    "samples_out.loc[samples_out.index.isin(located_samples['Lab_SampleID']), 'Confidence_Code'] = -2\n",
    "print(len(located_samples), \"samples match at least one parcel\")\n",
    "\n",
    "# Set the right coordinate system (NAD27 UTM 15N, EPSG 26915)\n",
    "located_samples = located_samples.to_crs(26915)\n",
    "located_samples.to_file(output_dir / 'located_many.gpkg')\n",
    "\n",
    "# Remove uncessary columns, drop cases where multiple parcels have the same RR address\n",
    "multiple_parcels = located_samples[located_samples.duplicated(subset=['Lab_SampleID'], keep=False)]\n",
    "\n",
    "multiple_parcels.to_file(output_dir / 'multiple_parcels.gpkg')\n",
    "print(multiple_parcels[\"Lab_SampleID\"])\n",
    "located_samples = located_samples[['Lab_SampleID', 'SampleAddress', 'geometry']].drop_duplicates(subset='Lab_SampleID', keep=False)\n",
    "\n",
    "print(len(multiple_parcels))\n",
    "\n",
    "# Assign a confidence code of -2 to samples where multiple parcel polygons match the address\n",
    "#samples_out.loc[multiple_parcels[\"Lab_SampleID\"], 'Confidence_Code'] = -2\n",
    "#print(len(samples_out[samples_out['Confidence_Code'] == -2]))\n",
    "\n",
    "print(len(located_samples), \"samples match exactly one parcel\")\n",
    "#samples_out.to_csv(output_dir / 'samples_out_test.csv')\n",
    "\n",
    "#TODO: 134 in confidence code vs 291 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match wells to parcels with associated samples using an intersect test\n",
    "sample_well_match = gpd.sjoin(located_samples, wells, how=\"inner\", predicate=\"intersects\")\n",
    "print(len(sample_well_match))\n",
    "sample_well_match.to_file(output_dir / 'sample_well_match.gpkg')\n",
    "\n",
    "# Drop rows where the year sampled is before the year drilled\n",
    "# unknown drill dates have a value of '0' and therefore survive this check\n",
    "\n",
    "# Check if the sample ID contains a sample year, otherwise skip this check\n",
    "date_checkable_samples = sample_well_match[sample_well_match['Lab_SampleID'].str.contains('\\d{4}-\\d{2}-\\d{4}', regex=True)]\n",
    "non_date_checkable_samples = sample_well_match[~sample_well_match['Lab_SampleID'].str.contains('\\d{4}-\\d{2}-\\d{4}', regex=True)] # There's probably an easier way of just taking the difference between the last df and the df with everything in it\n",
    "print(len(date_checkable_samples))\n",
    "print(len(non_date_checkable_samples))\n",
    "\n",
    "sample_well_date_match = date_checkable_samples[date_checkable_samples['Lab_SampleID'].str.slice(0,4) >= date_checkable_samples['DATE_DRLL'].astype(str).str.slice(0,4)]\n",
    "invalid_dates = date_checkable_samples[date_checkable_samples['Lab_SampleID'].str.slice(0,4) < date_checkable_samples['DATE_DRLL'].astype(str).str.slice(0,4)]\n",
    "print(len(invalid_dates))\n",
    "# Assign a confidence code of -4 for samples where the only wells on the matching property were drilled after the sample date\n",
    "samples_out.loc[invalid_dates['Lab_SampleID'], 'Confidence_Code'] = -4\n",
    "\n",
    "passed_samples = pd.concat([sample_well_date_match, non_date_checkable_samples])\n",
    "print(len(passed_samples))\n",
    "\n",
    "sample_well_date_match.to_file(output_dir / 'sample_well_match_datechecked.gpkg')\n",
    "\n",
    "\n",
    "# Assign a confidence code of -3 to samples where multiple wells may match the sample\n",
    "multiple_wells = passed_samples[passed_samples.duplicated(subset=['Lab_SampleID'], keep=False)]\n",
    "samples_out.loc[multiple_wells['Lab_SampleID'], 'Confidence_Code'] = -3 \n",
    "print(len(passed_samples), \"Hi\")\n",
    "\n",
    "# Filter to only necessary columns and remove cases where there are multiple wells on a parcel\n",
    "unique_sample_well_match = passed_samples[['Lab_SampleID', 'UTME', 'UTMN', 'WELLID', 'Verified', 'DATE_DRLL', 'geometry']].drop_duplicates(subset='Lab_SampleID', keep=False)\n",
    "print(len(unique_sample_well_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Export samples that can be confidently matched to a single well\n",
    "unique_sample_well_match.to_file(output_dir / 'unique_sample_well_match_911.gpkg')\n",
    "unique_sample_well_match.to_csv(output_dir / 'unique_sample_well_match_911.csv')\n",
    "\n",
    "# Assign a confidence code of 1 to samples that match with the highest confidence\n",
    "# Assign a confidence code of 2 to samples that may match, but have an unknown drill date\n",
    "unknown_date = unique_sample_well_match.loc[unique_sample_well_match['DATE_DRLL'] == 0]\n",
    "samples_out.loc[unique_sample_well_match['Lab_SampleID'], 'Confidence_Code'] = 1\n",
    "samples_out.loc[unknown_date['Lab_SampleID'], 'Confidence_Code'] = 2\n",
    "samples_out.to_csv(output_dir / 'samples_out_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Matches with at least one well:\", len(sample_well_match))\n",
    "print(\"Matches with exactly one well:\", len(unique_sample_well_match))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
